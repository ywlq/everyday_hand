{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./batchnorm_1d.png\" width=\"300\" height=\"400\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./batchnorm_2d.png\" width=\"300\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./batchnorm_eq.png\" width=\"200\" height=\"100\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思路：MLP中在特征维度归一化，CNN中在channel维度归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    # num_features：完全连接层的输出数量或卷积层的输出通道数。\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super().__init__()\n",
    "        if num_dims == 2: # 全连接层\n",
    "            shape = (1, num_features)\n",
    "        else:             # 卷积层\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        \n",
    "        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成1和0\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        \n",
    "        # 非模型参数的变量初始化为0和1\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.ones(shape)\n",
    "\n",
    "    def batch_norm(self, X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "        if not torch.is_grad_enabled():\n",
    "            # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差\n",
    "            X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "        else:\n",
    "            assert len(X.shape) in (2, 4)\n",
    "            if len(X.shape) == 2:\n",
    "                # 使用全连接层的情况，计算特征维上的均值和方差\n",
    "                mean = X.mean(dim=0)                                       # (num_features,)\n",
    "                var = ((X - mean) ** 2).mean(dim=0)                        # (num_features,)\n",
    "            else:\n",
    "                # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。\n",
    "                mean = X.mean(dim=(0, 2, 3), keepdim=True)                # (1,num_features,1,1) 保持X的形状，以便后面可以做广播运算\n",
    "                var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True) # (1,num_features,1,1)\n",
    "                \n",
    "            # 训练模式下，用当前的均值和方差做标准化\n",
    "            X_hat = (X - mean) / torch.sqrt(var + eps)\n",
    "            \n",
    "            # 更新移动平均的均值和方差\n",
    "            moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
    "            moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
    "            \n",
    "        Y = gamma * X_hat + beta  # 缩放和移位\n",
    "        return Y, moving_mean.data, moving_var.data\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 如果X不在内存上，将moving_mean和moving_var，复制到X所在显存上\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "       \n",
    "        # 保存更新过的moving_mean和moving_var\n",
    "        Y, self.moving_mean, self.moving_var = self.batch_norm(\n",
    "            X, self.gamma, self.beta, self.moving_mean,\n",
    "            self.moving_var, eps=1e-5, momentum=0.9\n",
    "        )\n",
    "        return Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LatchNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./layernorm_1d.png\" width=\"300\" height=\"400\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./layernorm_2d.png\" width=\"300\" height=\"400\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./batchnorm_eq.png\" width=\"200\" height=\"100\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思路：对一个batch中的输入归一化，对每个token的feature进行归一化，缩放到了统一的尺度，随后对每个特征维度施加统一的拉伸和偏移"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LayerNorm 层有两种放置方式：Pre Norm和Post Norm  \n",
    "<img src=\"./two_layernorm.png\" width=\"300\" height=\"400\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同一设置之下，Pre Norm结构往往更容易训练，但最终效果通常不如Post Norm\n",
    "1. Pre Norm 更容易训练好理解，因为它的恒等路径更突出\n",
    "2. Pre Norm 中多层叠加的结果更多是增加宽度而不是深度，层数越多，这个层就越“虚”，这是因为 Pre Norm 结构无形地增加了模型的宽度而降低了模型的深度，而我们知道深度通常比宽度更重要，所以是无形之中的降低深度导致最终效果变差了。而 Post Norm 刚刚相反，它每Norm一次就削弱一次恒等分支的权重，所以 Post Norm 反而是更突出残差分支的，因此Post Norm中的层数更加有分量，起到了作用，一旦训练好之后效果更优。\n",
    "\n",
    "过去 BERT 主流的时代往往使用 Post Norm，现在 GPT 时代模型规模都很大，因此大多用 Pre Norm 来稳定训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./RMSNorm.png\" width=\"500\" height=\"100\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 思路：layernorm的变体，简单看作均值为0，减少了计算均值的步骤，减少了运算，也没有拉升操作，减少了参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaRMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        LlamaRMSNorm is equivalent to T5LayerNorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states.to(input_dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lfdy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
