{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiHead Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./multihead_attention.png\" width=\"500\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1877,  0.1657, -0.0410,  ..., -0.2167, -0.0197,  0.1195],\n",
      "         [ 0.0885,  0.1367, -0.0774,  ..., -0.1746, -0.0669,  0.1598],\n",
      "         [ 0.1100,  0.1292, -0.0874,  ..., -0.1688, -0.0893,  0.2096],\n",
      "         ...,\n",
      "         [ 0.0604,  0.1330, -0.1173,  ..., -0.0713, -0.0913,  0.1809],\n",
      "         [ 0.0596,  0.1345, -0.1183,  ..., -0.0724, -0.0928,  0.1801],\n",
      "         [ 0.0593,  0.1320, -0.1162,  ..., -0.0743, -0.0932,  0.1779]],\n",
      "\n",
      "        [[ 0.0258, -0.1764, -0.1212,  ..., -0.0511, -0.2387,  0.2261],\n",
      "         [ 0.1300,  0.0085, -0.1791,  ..., -0.1085, -0.2460,  0.1905],\n",
      "         [ 0.0437,  0.0695, -0.1528,  ..., -0.0671, -0.2192,  0.1942],\n",
      "         ...,\n",
      "         [ 0.0515,  0.1371, -0.1193,  ..., -0.0898, -0.1102,  0.1712],\n",
      "         [ 0.0509,  0.1384, -0.1182,  ..., -0.0896, -0.1107,  0.1692],\n",
      "         [ 0.0517,  0.1390, -0.1204,  ..., -0.0894, -0.1110,  0.1687]],\n",
      "\n",
      "        [[-0.0492,  0.1312, -0.1478,  ..., -0.1601, -0.2125,  0.2068],\n",
      "         [ 0.0322,  0.0366, -0.1446,  ..., -0.1937, -0.1161,  0.1733],\n",
      "         [ 0.0449,  0.0456, -0.1661,  ..., -0.1213, -0.0798,  0.1879],\n",
      "         ...,\n",
      "         [ 0.0683,  0.1394, -0.1388,  ..., -0.0869, -0.0893,  0.1846],\n",
      "         [ 0.0670,  0.1423, -0.1389,  ..., -0.0886, -0.0883,  0.1835],\n",
      "         [ 0.0661,  0.1414, -0.1378,  ..., -0.0885, -0.0868,  0.1844]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0809,  0.1235, -0.1739,  ..., -0.1305, -0.0879,  0.0662],\n",
      "         [ 0.1195,  0.1769, -0.1251,  ..., -0.1470, -0.0172,  0.1392],\n",
      "         [ 0.0758,  0.1072, -0.1568,  ..., -0.1060, -0.0563,  0.1329],\n",
      "         ...,\n",
      "         [ 0.0733,  0.1251, -0.1483,  ..., -0.0733, -0.1168,  0.1941],\n",
      "         [ 0.0703,  0.1249, -0.1508,  ..., -0.0720, -0.1148,  0.1948],\n",
      "         [ 0.0744,  0.1225, -0.1508,  ..., -0.0718, -0.1167,  0.1946]],\n",
      "\n",
      "        [[ 0.0589,  0.3154, -0.0283,  ..., -0.0676, -0.0291,  0.2419],\n",
      "         [ 0.0678,  0.2915, -0.0804,  ..., -0.0857, -0.0053,  0.2197],\n",
      "         [ 0.0938,  0.1856, -0.1422,  ..., -0.0614, -0.0186,  0.2846],\n",
      "         ...,\n",
      "         [ 0.0764,  0.1199, -0.1276,  ..., -0.0629, -0.0686,  0.1884],\n",
      "         [ 0.0764,  0.1217, -0.1290,  ..., -0.0626, -0.0692,  0.1881],\n",
      "         [ 0.0765,  0.1234, -0.1296,  ..., -0.0625, -0.0705,  0.1861]],\n",
      "\n",
      "        [[ 0.1217, -0.0823, -0.3272,  ..., -0.1604, -0.0496,  0.2942],\n",
      "         [ 0.1749,  0.0093, -0.1677,  ..., -0.0290, -0.0503,  0.2679],\n",
      "         [ 0.1512, -0.0132, -0.1299,  ..., -0.0996, -0.0253,  0.2733],\n",
      "         ...,\n",
      "         [ 0.0534,  0.1294, -0.1234,  ..., -0.0689, -0.0919,  0.1867],\n",
      "         [ 0.0558,  0.1289, -0.1226,  ..., -0.0683, -0.0917,  0.1894],\n",
      "         [ 0.0552,  0.1293, -0.1231,  ..., -0.0689, -0.0935,  0.1874]]],\n",
      "       grad_fn=<ViewBackward0>) torch.Size([128, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.n_d = d_model // n_head\n",
    "\n",
    "        self.w_q= nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        batch_size,time,dimension=q.shape\n",
    "\n",
    "        q,k,v=self.w_q(q),self.w_k(k),self.w_v(v)\n",
    "        q=q.view(batch_size,time,self.n_head,self.n_d).permute(0,2,1,3)\n",
    "        k=k.view(batch_size,time,self.n_head,self.n_d).permute(0,2,1,3)\n",
    "        v=v.view(batch_size,time,self.n_head,self.n_d).permute(0,2,1,3)\n",
    "\n",
    "        score=q@k.transpose(2,3)/math.sqrt(self.n_d)\n",
    "\n",
    "        mask=torch.tril(torch.ones(time,time,dtype=bool))\n",
    "        score=score.masked_fill(mask==0,float(\"-inf\"))\n",
    "\n",
    "        out=F.softmax(score,dim=-1)@v\n",
    "        out=out.permute(0,2,1,3).contiguous().view(batch_size,time,self.d_model)\n",
    "        out=self.w_o(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "X=torch.rand(128,64,512)\n",
    "d_model=512 \n",
    "n_head=8\n",
    "\n",
    "attention=MultiHeadAttention(d_model,n_head)\n",
    "output=attention(X,X,X)\n",
    "print(output,output.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_factory_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
