{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiHead Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/multihead_attention.png\" width=\"500\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.n_d = d_model // n_head\n",
    "\n",
    "        self.w_q= nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size,time,dimension=q.shape\n",
    "\n",
    "        q,k,v=self.w_q(q),self.w_k(k),self.w_v(v)\n",
    "        q=q.view(batch_size,time,self.n_head,self.n_d).permute(0,2,1,3)\n",
    "        k=k.view(batch_size,time,self.n_head,self.n_d).permute(0,2,1,3)\n",
    "        v=v.view(batch_size,time,self.n_head,self.n_d).permute(0,2,1,3)\n",
    "\n",
    "        score=q@k.transpose(2,3)/math.sqrt(self.n_d)\n",
    "        if mask is not None:\n",
    "            mask=torch.tril(torch.ones(time,time,dtype=bool))\n",
    "            score=score.masked_fill(mask==0,1e-9)\n",
    "\n",
    "        out=F.softmax(score,dim=-1)@v\n",
    "        out=out.permute(0,2,1,3).contiguous().view(batch_size,time,self.d_model)\n",
    "        out=self.w_o(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "X=torch.rand(128,64,512)\n",
    "d_model=512 \n",
    "n_head=8\n",
    "\n",
    "attention=MultiHeadAttention(d_model,n_head)\n",
    "output=attention(X,X,X)\n",
    "#print(output,output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "## TokenEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.token_embedding(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PositionalEbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/positional_encoding.jpg\" width=\"500\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, device):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self.device = device\n",
    "\n",
    "        self.pe = torch.zeros(max_len, d_model, devide=device)\n",
    "        self.pe.requires_grad_(False)\n",
    "\n",
    "        for pos in range(max_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                self.pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                self.pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len=x.shape[1]\n",
    "        return self.pe[:seq_len, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Totol Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len, drop_prob , device):\n",
    "        super(TransformerEmbedding, self).__init__()\n",
    "        self.token_embedding = TokenEmbedding(vocab_size, d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model, max_len, device)\n",
    "        self.drop_out= nn.Dropout(drop_prob)\n",
    "        \n",
    "\n",
    "    def forward(self, x): \n",
    "        tok_emb= self.token_embedding(x)\n",
    "        pos_emb = self.position_embedding(x)\n",
    "        return self.drop_out(tok_emb + pos_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/layer_norm.jpg\" width=\"500\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Norm 是在通道维度上做归一化，即把每个样本的对应的通道都拿出来单独做归一化\n",
    "Layer Norm 是在样本维度上做归一化，即把每个样本都拿出来单独做归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.eps = eps\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean=x.mean(dim=-1, keepdim=True)\n",
    "        var=x.var(dim=-1,unbiased=False,keepdim=True)\n",
    "        out=(x-mean)/torch.sqrt(var+self.eps)\n",
    "        out=self.gamma*out+self.beta\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/positionwise_feed_forward.jpg\" width=\"500\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.hidden = hidden\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.fc1 = nn.Linear(d_model, hidden)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.fc1(x)\n",
    "        x=self.dropout(F.relu(x))\n",
    "        x=self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/transformer_resideual_layer_norm_3.png\" width=\"500\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden ,n_head, drop_prob):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.ffn_hidden = ffn_hidden\n",
    "        self.n_head = n_head\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        self.attention = MultiHeadAttention(d_model, n_head)\n",
    "        self.norm1= LayerNorm(d_model)\n",
    "        self.drop1= nn.Dropout(drop_prob)\n",
    "\n",
    "        self.ffn= PositionwiseFeedForward(d_model, ffn_hidden,drop_prob)\n",
    "        self.norm2= LayerNorm(d_model)\n",
    "        self.drop2= nn.Dropout(drop_prob)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        _x=x\n",
    "        x=self.attention(x,x,x,mask)\n",
    "\n",
    "        x=self.norm1(x+_x)\n",
    "        x=self.drop1(x)\n",
    "\n",
    "        _x=x\n",
    "        x=self.ffn(x)\n",
    "\n",
    "        x=self.norm2(x+_x)\n",
    "        x=self.drop2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_factory_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
